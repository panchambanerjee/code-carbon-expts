{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOp0F3MDEPOayX8DzjkLh4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panchambanerjee/code-carbon-expts/blob/main/code_carbon_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlXoYby1O5-F",
        "outputId": "88dfb6a1-2fd6-4388-f260-df18d75e8387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.6/549.6 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.1/510.1 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate huggingface_hub codecarbon torch comet_ml -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile comet_codecarbon_mnist.py\n",
        "\n",
        "from comet_ml import Experiment  # isort:skip\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assumes Comet variable environment configuration.\n",
        "# See here to get your API_KEY:\n",
        "# https://www.comet.ml/user/settings/account#section-DEVELOPER_INFORMATION\n",
        "# And here for setup information:\n",
        "# https://www.comet.ml/docs/python-sdk/advanced/#experiment-configuration-parameters\n",
        "experiment = Experiment(api_key='')\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAFEhQCra1-X",
        "outputId": "a55bc9b2-854d-4036-c398-b61db0159828"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting comet_codecarbon_mnist.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python comet_codecarbon_mnist.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY6Qhezmbtx1",
        "outputId": "3a13625c-df1e-45ce-812b-6dc06578e9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-26 18:54:28.138279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-26 18:54:29.108808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[codecarbon INFO @ 18:54:31] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 18:54:31] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 18:54:31] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 18:54:31] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 18:54:31] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon WARNING @ 18:54:33] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 18:54:33] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 18:54:33] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 18:54:33]   Platform system: Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 18:54:33]   Python version: 3.10.6\n",
            "[codecarbon INFO @ 18:54:33]   CodeCarbon version: 2.2.7\n",
            "[codecarbon INFO @ 18:54:33]   Available RAM : 12.678 GB\n",
            "[codecarbon INFO @ 18:54:33]   CPU count: 2\n",
            "[codecarbon INFO @ 18:54:33]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 18:54:33]   GPU count: 1\n",
            "[codecarbon INFO @ 18:54:33]   GPU model: 1 x Tesla V100-SXM2-16GB\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/panchambanerjee/general/35663a42442c44f0811f54e01b5c2a80\u001b[0m\n",
            "\n",
            "2023-07-26 18:54:35.647706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:35.988727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:35.989114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:35.990780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:35.991038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:35.991269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:38.817697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:38.818102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:38.818450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-26 18:54:38.818642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-07-26 18:54:38.818707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14504 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "2023-07-26 18:54:39.125045: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
            "2023-07-26 18:54:39.355264: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\n",
            "Epoch 1/10\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/comet_codecarbon_mnist.py\", line 31, in <module>\n",
            "    model.fit(x_train, y_train, epochs=10)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/comet_ml/monkey_patching.py\", line 324, in wrapper\n",
            "    return_value = original(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 894, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 959, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 142, in __call__\n",
            "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 396, in _maybe_define_function\n",
            "    concrete_function = self._create_concrete_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 300, in _create_concrete_function\n",
            "    func_graph_module.func_graph_from_py_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 1214, in func_graph_from_py_func\n",
            "    func_outputs = python_func(*func_args, **func_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 667, in wrapped_fn\n",
            "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 1189, in autograph_handler\n",
            "    return autograph.converted_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
            "    result = converted_f(*effective_args, **kwargs)\n",
            "  File \"/tmp/__autograph_generated_fileu5ak_jmw.py\", line 15, in tf__train_function\n",
            "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 331, in converted_call\n",
            "    return _call_unconverted(f, args, kwargs, options, False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
            "    return f(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n",
            "    outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1316, in run\n",
            "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 2895, in call_for_each_replica\n",
            "    return self._call_for_each_replica(fn, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 3696, in _call_for_each_replica\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 689, in wrapper\n",
            "    return converted_call(f, args, kwargs, options=options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\n",
            "    return _call_unconverted(f, args, kwargs, options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 458, in _call_unconverted\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1252, in run_step\n",
            "    model._train_counter.assign_add(1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 939, in assign_add\n",
            "    ops.convert_to_tensor(delta, dtype=self.dtype),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\", line 183, in wrapped\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 1642, in convert_to_tensor\n",
            "    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 48, in _default_conversion_function\n",
            "    return constant_op.constant(value, dtype, name=name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\", line 268, in constant\n",
            "    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\", line 290, in _constant_impl\n",
            "    const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 707, in _create_op_internal\n",
            "    return super()._create_op_internal(  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 3807, in _create_op_internal\n",
            "    node_def = _NodeDef(op_type, name, attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 1902, in _NodeDef\n",
            "    node_def.attr[k].CopyFrom(v)\n",
            "KeyboardInterrupt\n",
            "[codecarbon INFO @ 18:54:48] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:54:48] Energy consumed for all GPUs : 0.000172 kWh. Total GPU Power : 41.265 W\n",
            "[codecarbon INFO @ 18:54:48] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:54:48] 0.000369 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:55:03] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:55:03] Energy consumed for all GPUs : 0.000344 kWh. Total GPU Power : 41.265 W\n",
            "[codecarbon INFO @ 18:55:03] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:55:03] 0.000738 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:55:18] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:55:18] Energy consumed for all GPUs : 0.000516 kWh. Total GPU Power : 41.265 W\n",
            "[codecarbon INFO @ 18:55:18] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:55:18] 0.001107 kWh of electricity used since the beginning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "t5EnzFWQQnwq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X8giaC-Q61R",
        "outputId": "bcd42b26-f5a7-4c72-d74a-01ef3f74f3c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "Jhi9cos4RKPl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert Base Uncased"
      ],
      "metadata": {
        "id": "TCbg_jy-Rmzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Compute intensive code goes here\n",
        "\n",
        "ds = load_dataset(\"imdb\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "small_train_dataset = ds[\"train\"].shuffle(seed=42).\\\n",
        "select(range(1000)).map(tokenize_function, batched=True)\n",
        "\n",
        "small_eval_dataset = ds[\"test\"].shuffle(seed=42).\\\n",
        "select(range(1000)).map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/code-carbon-classification\",\n",
        "    num_train_epochs=1,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Compute intensive code stops here\n",
        "\n",
        "tracker.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "NY7IF1sIRVHZ",
        "outputId": "1b0687aa-dda4-4c53-fe0c-ff565cf46f95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a0d5c4a68744>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmissionsTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute intensive code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EmissionsTracker' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bert-base-cased-codecarbon-comet-run.py\n",
        "\n",
        "from comet_ml import Experiment\n",
        "\n",
        "experiment = Experiment(api_key='')\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Compute intensive code goes here\n",
        "\n",
        "ds = load_dataset(\"imdb\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "\n",
        "small_train_dataset = ds[\"train\"].shuffle(seed=42).\\\n",
        "select(range(1000)).map(tokenize_function, batched=True)\n",
        "\n",
        "small_eval_dataset = ds[\"test\"].shuffle(seed=42).\\\n",
        "select(range(1000)).map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output_dir\",\n",
        "    num_train_epochs=10,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwU8DB5wRsou",
        "outputId": "dd238b50-2543-49d2-8549-a037b1ea7587"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bert-base-cased-codecarbon-comet-run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bert-base-cased-codecarbon-comet-run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAeFrmfCfHoI",
        "outputId": "e0e937ec-1a2f-47e4-a102-65e685bcc7fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[codecarbon INFO @ 18:39:11] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 18:39:11] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 18:39:11] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 18:39:11] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 18:39:11] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon WARNING @ 18:39:13] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 18:39:13] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 18:39:13] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 18:39:13]   Platform system: Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 18:39:13]   Python version: 3.10.6\n",
            "[codecarbon INFO @ 18:39:13]   CodeCarbon version: 2.2.7\n",
            "[codecarbon INFO @ 18:39:13]   Available RAM : 12.678 GB\n",
            "[codecarbon INFO @ 18:39:13]   CPU count: 2\n",
            "[codecarbon INFO @ 18:39:13]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 18:39:13]   GPU count: 1\n",
            "[codecarbon INFO @ 18:39:13]   GPU model: 1 x Tesla V100-SXM2-16GB\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/panchambanerjee/general/88393556e909432983a823fed080d337\u001b[0m\n",
            "\n",
            "comet_ml is installed but `COMET_API_KEY` is not set.\n",
            "2023-07-26 18:39:17.406074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Generating train split:  98%|█████████▊| 24497/25000 [00:06<00:00, 9111.91 examples/s][codecarbon INFO @ 18:39:28] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:39:28] Energy consumed for all GPUs : 0.000108 kWh. Total GPU Power : 25.899 W\n",
            "[codecarbon INFO @ 18:39:28] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:39:28] 0.000305 kWh of electricity used since the beginning.\n",
            "Generating train split: 100%|██████████| 25000/25000 [00:09<00:00, 2624.74 examples/s]\n",
            "Generating test split: 100%|██████████| 25000/25000 [00:07<00:00, 3231.80 examples/s]\n",
            "Generating unsupervised split:  35%|███▍      | 17317/50000 [00:06<00:03, 8851.61 examples/s][codecarbon INFO @ 18:39:43] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:39:43] Energy consumed for all GPUs : 0.000216 kWh. Total GPU Power : 25.899 W\n",
            "[codecarbon INFO @ 18:39:43] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:39:43] 0.000610 kWh of electricity used since the beginning.\n",
            "Generating unsupervised split: 100%|██████████| 50000/50000 [00:09<00:00, 5121.58 examples/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 3.12MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 436M/436M [00:05<00:00, 82.7MB/s]\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 98.6kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 8.26MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 23.3MB/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1085.11 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1260.29 examples/s]\n",
            "[codecarbon INFO @ 18:39:58] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:39:58] Energy consumed for all GPUs : 0.000386 kWh. Total GPU Power : 40.77 W\n",
            "[codecarbon INFO @ 18:39:58] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:39:58] 0.000976 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:40:02] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 18:40:02] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 18:40:02] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 18:40:02] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 18:40:02] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon WARNING @ 18:40:03] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 18:40:03] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 18:40:03] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 18:40:03]   Platform system: Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 18:40:03]   Python version: 3.10.6\n",
            "[codecarbon INFO @ 18:40:03]   CodeCarbon version: 2.2.7\n",
            "[codecarbon INFO @ 18:40:03]   Available RAM : 12.678 GB\n",
            "[codecarbon INFO @ 18:40:03]   CPU count: 2\n",
            "[codecarbon INFO @ 18:40:03]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 18:40:03]   GPU count: 1\n",
            "[codecarbon INFO @ 18:40:03]   GPU model: 1 x Tesla V100-SXM2-16GB\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  2%|▏         | 24/1250 [00:09<05:02,  4.05it/s][codecarbon INFO @ 18:40:13] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:13] Energy consumed for all GPUs : 0.001124 kWh. Total GPU Power : 177.268 W\n",
            "[codecarbon INFO @ 18:40:13] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:13] 0.001912 kWh of electricity used since the beginning.\n",
            "  4%|▍         | 48/1250 [00:14<04:56,  4.06it/s][codecarbon INFO @ 18:40:19] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:19] Energy consumed for all GPUs : 0.001141 kWh. Total GPU Power : 273.721 W\n",
            "[codecarbon INFO @ 18:40:19] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:19] 0.001338 kWh of electricity used since the beginning.\n",
            "  7%|▋         | 84/1250 [00:24<04:48,  4.04it/s][codecarbon INFO @ 18:40:28] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:28] Energy consumed for all GPUs : 0.002230 kWh. Total GPU Power : 265.544 W\n",
            "[codecarbon INFO @ 18:40:28] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:28] 0.003215 kWh of electricity used since the beginning.\n",
            "  9%|▊         | 107/1250 [00:29<04:41,  4.06it/s][codecarbon INFO @ 18:40:34] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:34] Energy consumed for all GPUs : 0.002301 kWh. Total GPU Power : 278.504 W\n",
            "[codecarbon INFO @ 18:40:34] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:34] 0.002695 kWh of electricity used since the beginning.\n",
            " 11%|█▏        | 143/1250 [00:39<04:32,  4.06it/s][codecarbon INFO @ 18:40:43] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:43] Energy consumed for all GPUs : 0.003316 kWh. Total GPU Power : 260.628 W\n",
            "[codecarbon INFO @ 18:40:43] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:43] 0.004497 kWh of electricity used since the beginning.\n",
            " 13%|█▎        | 166/1250 [00:44<04:27,  4.05it/s][codecarbon INFO @ 18:40:49] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:49] Energy consumed for all GPUs : 0.003201 kWh. Total GPU Power : 216.181 W\n",
            "[codecarbon INFO @ 18:40:49] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:49] 0.003792 kWh of electricity used since the beginning.\n",
            " 16%|█▌        | 202/1250 [00:54<04:20,  4.02it/s][codecarbon INFO @ 18:40:58] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:40:58] Energy consumed for all GPUs : 0.003694 kWh. Total GPU Power : 90.657 W\n",
            "[codecarbon INFO @ 18:40:58] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:40:58] 0.005072 kWh of electricity used since the beginning.\n",
            " 18%|█▊        | 225/1250 [00:59<04:13,  4.05it/s][codecarbon INFO @ 18:41:04] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:04] Energy consumed for all GPUs : 0.004122 kWh. Total GPU Power : 221.103 W\n",
            "[codecarbon INFO @ 18:41:04] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:04] 0.004910 kWh of electricity used since the beginning.\n",
            " 21%|██        | 261/1250 [01:09<04:07,  4.00it/s][codecarbon INFO @ 18:41:13] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:13] Energy consumed for all GPUs : 0.004824 kWh. Total GPU Power : 271.619 W\n",
            "[codecarbon INFO @ 18:41:13] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:13] 0.006400 kWh of electricity used since the beginning.\n",
            " 23%|██▎       | 284/1250 [01:14<03:57,  4.06it/s][codecarbon INFO @ 18:41:19] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:19] Energy consumed for all GPUs : 0.005271 kWh. Total GPU Power : 275.866 W\n",
            "[codecarbon INFO @ 18:41:19] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:19] 0.006256 kWh of electricity used since the beginning.\n",
            " 26%|██▌       | 320/1250 [01:24<03:49,  4.05it/s][codecarbon INFO @ 18:41:28] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:28] Energy consumed for all GPUs : 0.006068 kWh. Total GPU Power : 298.476 W\n",
            "[codecarbon INFO @ 18:41:28] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:28] 0.007840 kWh of electricity used since the beginning.\n",
            " 27%|██▋       | 343/1250 [01:29<03:45,  4.03it/s][codecarbon INFO @ 18:41:34] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:34] Energy consumed for all GPUs : 0.006486 kWh. Total GPU Power : 291.594 W\n",
            "[codecarbon INFO @ 18:41:34] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:34] 0.007667 kWh of electricity used since the beginning.\n",
            " 30%|███       | 379/1250 [01:39<03:37,  4.01it/s][codecarbon INFO @ 18:41:43] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:43] Energy consumed for all GPUs : 0.007257 kWh. Total GPU Power : 285.324 W\n",
            "[codecarbon INFO @ 18:41:43] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:43] 0.009226 kWh of electricity used since the beginning.\n",
            " 32%|███▏      | 402/1250 [01:44<03:31,  4.02it/s][codecarbon INFO @ 18:41:49] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:49] Energy consumed for all GPUs : 0.007620 kWh. Total GPU Power : 272.11 W\n",
            "[codecarbon INFO @ 18:41:49] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:49] 0.008998 kWh of electricity used since the beginning.\n",
            " 35%|███▌      | 438/1250 [01:54<03:20,  4.05it/s][codecarbon INFO @ 18:41:58] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:41:58] Energy consumed for all GPUs : 0.008319 kWh. Total GPU Power : 255.062 W\n",
            "[codecarbon INFO @ 18:41:58] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:41:58] 0.010485 kWh of electricity used since the beginning.\n",
            " 37%|███▋      | 461/1250 [01:59<03:15,  4.04it/s][codecarbon INFO @ 18:42:04] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:04] Energy consumed for all GPUs : 0.008702 kWh. Total GPU Power : 259.814 W\n",
            "[codecarbon INFO @ 18:42:04] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:04] 0.010277 kWh of electricity used since the beginning.\n",
            " 40%|███▉      | 497/1250 [02:09<03:06,  4.03it/s][codecarbon INFO @ 18:42:13] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:13] Energy consumed for all GPUs : 0.009477 kWh. Total GPU Power : 277.83200000000005 W\n",
            "[codecarbon INFO @ 18:42:13] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:13] 0.011840 kWh of electricity used since the beginning.\n",
            " 40%|████      | 500/1250 [02:09<03:04,  4.07it/s]\n",
            " 40%|████      | 501/1250 [02:14<20:17,  1.62s/it][codecarbon INFO @ 18:42:19] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:19] Energy consumed for all GPUs : 0.009649 kWh. Total GPU Power : 227.304 W\n",
            "[codecarbon INFO @ 18:42:19] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:19] 0.011421 kWh of electricity used since the beginning.\n",
            " 43%|████▎     | 537/1250 [02:24<02:56,  4.05it/s][codecarbon INFO @ 18:42:28] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:28] Energy consumed for all GPUs : 0.010595 kWh. Total GPU Power : 268.318 W\n",
            "[codecarbon INFO @ 18:42:28] Energy consumed for all CPUs : 0.002302 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:28] 0.013154 kWh of electricity used since the beginning.\n",
            " 45%|████▍     | 559/1250 [02:29<02:52,  4.01it/s][codecarbon INFO @ 18:42:34] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:34] Energy consumed for all GPUs : 0.010761 kWh. Total GPU Power : 267.01800000000003 W\n",
            "[codecarbon INFO @ 18:42:34] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:34] 0.012730 kWh of electricity used since the beginning.\n",
            " 48%|████▊     | 595/1250 [02:39<02:42,  4.03it/s][codecarbon INFO @ 18:42:43] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:43] Energy consumed for all GPUs : 0.011788 kWh. Total GPU Power : 286.374 W\n",
            "[codecarbon INFO @ 18:42:43] Energy consumed for all CPUs : 0.002479 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:43] 0.014544 kWh of electricity used since the beginning.\n",
            " 49%|████▉     | 618/1250 [02:44<02:36,  4.04it/s][codecarbon INFO @ 18:42:49] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:49] Energy consumed for all GPUs : 0.011944 kWh. Total GPU Power : 284.036 W\n",
            "[codecarbon INFO @ 18:42:49] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:49] 0.014110 kWh of electricity used since the beginning.\n",
            " 52%|█████▏    | 654/1250 [02:54<02:27,  4.04it/s][codecarbon INFO @ 18:42:58] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:42:58] Energy consumed for all GPUs : 0.012934 kWh. Total GPU Power : 275.062 W\n",
            "[codecarbon INFO @ 18:42:58] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:42:58] 0.015887 kWh of electricity used since the beginning.\n",
            " 54%|█████▍    | 677/1250 [02:59<02:22,  4.01it/s][codecarbon INFO @ 18:43:04] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:04] Energy consumed for all GPUs : 0.013076 kWh. Total GPU Power : 271.579 W\n",
            "[codecarbon INFO @ 18:43:04] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:04] 0.015439 kWh of electricity used since the beginning.\n",
            " 57%|█████▋    | 713/1250 [03:09<02:13,  4.01it/s][codecarbon INFO @ 18:43:13] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:13] Energy consumed for all GPUs : 0.014131 kWh. Total GPU Power : 287.358 W\n",
            "[codecarbon INFO @ 18:43:13] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:13] 0.017281 kWh of electricity used since the beginning.\n",
            " 59%|█████▉    | 736/1250 [03:14<02:08,  4.00it/s][codecarbon INFO @ 18:43:19] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:19] Energy consumed for all GPUs : 0.014188 kWh. Total GPU Power : 266.8740000000001 W\n",
            "[codecarbon INFO @ 18:43:19] Energy consumed for all CPUs : 0.002302 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:19] 0.016747 kWh of electricity used since the beginning.\n",
            " 62%|██████▏   | 772/1250 [03:24<01:58,  4.03it/s][codecarbon INFO @ 18:43:28] Energy consumed for RAM : 0.000337 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:28] Energy consumed for all GPUs : 0.015323 kWh. Total GPU Power : 286.119 W\n",
            "[codecarbon INFO @ 18:43:28] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:28] 0.018670 kWh of electricity used since the beginning.\n",
            " 64%|██████▎   | 795/1250 [03:29<01:53,  4.01it/s][codecarbon INFO @ 18:43:34] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:34] Energy consumed for all GPUs : 0.015440 kWh. Total GPU Power : 300.541 W\n",
            "[codecarbon INFO @ 18:43:34] Energy consumed for all CPUs : 0.002479 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:34] 0.018196 kWh of electricity used since the beginning.\n",
            " 66%|██████▋   | 831/1250 [03:39<01:44,  4.02it/s][codecarbon INFO @ 18:43:43] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:43] Energy consumed for all GPUs : 0.016296 kWh. Total GPU Power : 233.559 W\n",
            "[codecarbon INFO @ 18:43:43] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:43] 0.019840 kWh of electricity used since the beginning.\n",
            " 68%|██████▊   | 854/1250 [03:44<01:37,  4.05it/s][codecarbon INFO @ 18:43:49] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:49] Energy consumed for all GPUs : 0.016566 kWh. Total GPU Power : 270.495 W\n",
            "[codecarbon INFO @ 18:43:49] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:49] 0.019520 kWh of electricity used since the beginning.\n",
            " 71%|███████   | 890/1250 [03:54<01:29,  4.00it/s][codecarbon INFO @ 18:43:58] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:43:58] Energy consumed for all GPUs : 0.016870 kWh. Total GPU Power : 137.882 W\n",
            "[codecarbon INFO @ 18:43:58] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:43:58] 0.020612 kWh of electricity used since the beginning.\n",
            " 73%|███████▎  | 913/1250 [04:00<01:23,  4.02it/s][codecarbon INFO @ 18:44:04] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:04] Energy consumed for all GPUs : 0.017421 kWh. Total GPU Power : 205.261 W\n",
            "[codecarbon INFO @ 18:44:04] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:04] 0.020572 kWh of electricity used since the beginning.\n",
            " 76%|███████▌  | 948/1250 [04:09<01:15,  4.01it/s][codecarbon INFO @ 18:44:13] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:13] Energy consumed for all GPUs : 0.017696 kWh. Total GPU Power : 198.358 W\n",
            "[codecarbon INFO @ 18:44:13] Energy consumed for all CPUs : 0.003542 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:13] 0.021635 kWh of electricity used since the beginning.\n",
            " 78%|███████▊  | 970/1250 [04:14<01:13,  3.82it/s][codecarbon INFO @ 18:44:19] Energy consumed for RAM : 0.000337 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:19] Energy consumed for all GPUs : 0.018528 kWh. Total GPU Power : 265.717 W\n",
            "[codecarbon INFO @ 18:44:19] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:19] 0.021876 kWh of electricity used since the beginning.\n",
            "{'loss': 0.0217, 'learning_rate': 1e-05, 'epoch': 8.0}\n",
            " 80%|████████  | 1000/1250 [04:22<01:05,  3.82it/s][codecarbon INFO @ 18:44:28] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:28] Energy consumed for all GPUs : 0.017941 kWh. Total GPU Power : 58.08100000000001 W\n",
            "[codecarbon INFO @ 18:44:28] Energy consumed for all CPUs : 0.003722 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:28] 0.022079 kWh of electricity used since the beginning.\n",
            " 81%|████████  | 1011/1250 [04:29<01:08,  3.50it/s][codecarbon INFO @ 18:44:34] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:34] Energy consumed for all GPUs : 0.019628 kWh. Total GPU Power : 264.069 W\n",
            "[codecarbon INFO @ 18:44:34] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:34] 0.023173 kWh of electricity used since the beginning.\n",
            " 84%|████████▍ | 1047/1250 [04:39<00:51,  3.96it/s][codecarbon INFO @ 18:44:43] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:43] Energy consumed for all GPUs : 0.018644 kWh. Total GPU Power : 170.854 W\n",
            "[codecarbon INFO @ 18:44:43] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:43] 0.022978 kWh of electricity used since the beginning.\n",
            " 86%|████████▌ | 1070/1250 [04:44<00:44,  4.01it/s][codecarbon INFO @ 18:44:49] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:49] Energy consumed for all GPUs : 0.020808 kWh. Total GPU Power : 283.115 W\n",
            "[codecarbon INFO @ 18:44:49] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:49] 0.024549 kWh of electricity used since the beginning.\n",
            " 88%|████████▊ | 1106/1250 [04:54<00:35,  4.01it/s][codecarbon INFO @ 18:44:58] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:44:58] Energy consumed for all GPUs : 0.019438 kWh. Total GPU Power : 190.46800000000002 W\n",
            "[codecarbon INFO @ 18:44:58] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:44:58] 0.023968 kWh of electricity used since the beginning.\n",
            " 90%|█████████ | 1129/1250 [04:59<00:29,  4.05it/s][codecarbon INFO @ 18:45:04] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:04] Energy consumed for all GPUs : 0.021984 kWh. Total GPU Power : 282.255 W\n",
            "[codecarbon INFO @ 18:45:04] Energy consumed for all CPUs : 0.003542 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:04] 0.025922 kWh of electricity used since the beginning.\n",
            " 93%|█████████▎| 1165/1250 [05:09<00:21,  4.02it/s][codecarbon INFO @ 18:45:13] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:13] Energy consumed for all GPUs : 0.020180 kWh. Total GPU Power : 178.14000000000001 W\n",
            "[codecarbon INFO @ 18:45:13] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:13] 0.024907 kWh of electricity used since the beginning.\n",
            " 95%|█████████▌| 1188/1250 [05:14<00:15,  4.04it/s][codecarbon INFO @ 18:45:19] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:19] Energy consumed for all GPUs : 0.023064 kWh. Total GPU Power : 259.323 W\n",
            "[codecarbon INFO @ 18:45:19] Energy consumed for all CPUs : 0.003719 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:19] 0.027199 kWh of electricity used since the beginning.\n",
            " 98%|█████████▊| 1225/1250 [05:24<00:06,  4.04it/s][codecarbon INFO @ 18:45:28] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:28] Energy consumed for all GPUs : 0.021124 kWh. Total GPU Power : 226.517 W\n",
            "[codecarbon INFO @ 18:45:28] Energy consumed for all CPUs : 0.004429 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:28] 0.026047 kWh of electricity used since the beginning.\n",
            "100%|█████████▉| 1247/1250 [05:29<00:00,  4.04it/s][codecarbon INFO @ 18:45:34] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:34] Energy consumed for all GPUs : 0.024280 kWh. Total GPU Power : 291.8960000000001 W\n",
            "[codecarbon INFO @ 18:45:34] Energy consumed for all CPUs : 0.003896 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:34] 0.028612 kWh of electricity used since the beginning.\n",
            "{'train_runtime': 330.5996, 'train_samples_per_second': 30.248, 'train_steps_per_second': 3.781, 'train_loss': 0.1253167046070099, 'epoch': 10.0}\n",
            "100%|██████████| 1250/1250 [05:30<00:00,  4.05it/s][codecarbon INFO @ 18:45:34] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:34] Energy consumed for all GPUs : 0.024298 kWh. Total GPU Power : 114.899 W\n",
            "[codecarbon INFO @ 18:45:34] Energy consumed for all CPUs : 0.003903 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:34] 0.028637 kWh of electricity used since the beginning.\n",
            "100%|██████████| 1250/1250 [05:30<00:00,  3.78it/s]\n",
            "[codecarbon INFO @ 18:45:43] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:43] Energy consumed for all GPUs : 0.021312 kWh. Total GPU Power : 45.231 W\n",
            "[codecarbon INFO @ 18:45:43] Energy consumed for all CPUs : 0.004606 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:43] 0.026433 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:45:58] Energy consumed for RAM : 0.000535 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:45:58] Energy consumed for all GPUs : 0.021494 kWh. Total GPU Power : 43.744 W\n",
            "[codecarbon INFO @ 18:45:58] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:45:58] 0.026812 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:46:13] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:46:13] Energy consumed for all GPUs : 0.021672 kWh. Total GPU Power : 42.752 W\n",
            "[codecarbon INFO @ 18:46:13] Energy consumed for all CPUs : 0.004960 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:46:13] 0.027187 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:46:28] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:46:28] Energy consumed for all GPUs : 0.021848 kWh. Total GPU Power : 42.257 W\n",
            "[codecarbon INFO @ 18:46:28] Energy consumed for all CPUs : 0.005137 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:46:28] 0.027560 kWh of electricity used since the beginning.\n",
            "Exception ignored in: Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/comet_ml/console.py\", line 282, in write\n",
            "    self.__wrapped__.flush()\n",
            "KeyboardInterrupt\n",
            "[codecarbon INFO @ 18:46:37] Energy consumed for RAM : 0.000586 kWh. RAM Power : 4.754392147064209 W\n",
            "[codecarbon INFO @ 18:46:37] Energy consumed for all GPUs : 0.021953 kWh. Total GPU Power : 42.257 W\n",
            "[codecarbon INFO @ 18:46:37] Energy consumed for all CPUs : 0.005242 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 18:46:37] 0.027781 kWh of electricity used since the beginning.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/panchambanerjee/general/88393556e909432983a823fed080d337\u001b[0m\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [125]                     : (7.335406553465873e-05, 1.1188008785247803)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/epoch [3]                : (4.0, 10.0)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/learning_rate [2]        : (1e-05, 3e-05)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/loss [2]                 : (0.0217, 0.2902)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/total_flos               : 2631110553600000.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_loss               : 0.1253167046070099\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_runtime            : 330.5996\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_samples_per_second : 30.248\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_steps_per_second   : 3.781\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset               : 1 (764 bytes)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1 (1.03 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkoyfXdtfSzq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}